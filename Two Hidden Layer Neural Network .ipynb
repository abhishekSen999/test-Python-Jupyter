{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Hidden Layer Neural Network\n",
    "\n",
    "\n",
    "- Implementing a 2-class classification neural network with a two hidden layer\n",
    "- Using units with a non-linear activation function, such as tanh/sigmoid / relu \n",
    "- Computeing the cross entropy loss \n",
    "- Implementing forward and backward propagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "Let's first import all the packages that you will need during this assignment.\n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [sklearn](http://scikit-learn.org/stable/) provides simple and efficient tools for data mining and data analysis. \n",
    "- [matplotlib](http://matplotlib.org) is a library for plotting graphs in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import sklearn.linear_model\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1) # set a seed so that the results are consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Dataset ##\n",
    "\n",
    "First, let's get the dataset you will work on. The following code will load a \"flower\" 2-class dataset into variables `X` and `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module sklearn.datasets.olivetti_faces in sklearn.datasets:\n",
      "\n",
      "NAME\n",
      "    sklearn.datasets.olivetti_faces - Modified Olivetti faces dataset.\n",
      "\n",
      "DESCRIPTION\n",
      "    The original database was available from\n",
      "    \n",
      "        http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\n",
      "    \n",
      "    The version retrieved here comes in MATLAB format from the personal\n",
      "    web page of Sam Roweis:\n",
      "    \n",
      "        http://www.cs.nyu.edu/~roweis/\n",
      "    \n",
      "    There are ten different images of each of 40 distinct subjects. For some\n",
      "    subjects, the images were taken at different times, varying the lighting,\n",
      "    facial expressions (open / closed eyes, smiling / not smiling) and facial\n",
      "    details (glasses / no glasses). All the images were taken against a dark\n",
      "    homogeneous background with the subjects in an upright, frontal position (with\n",
      "    tolerance for some side movement).\n",
      "    \n",
      "    The original dataset consisted of 92 x 112, while the Roweis version\n",
      "    consists of 64x64 images.\n",
      "\n",
      "FUNCTIONS\n",
      "    fetch_olivetti_faces(data_home=None, shuffle=False, random_state=0, download_if_missing=True)\n",
      "        Loader for the Olivetti faces data-set from AT&T.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <olivetti_faces>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data_home : optional, default: None\n",
      "            Specify another download and cache folder for the datasets. By default\n",
      "            all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "        \n",
      "        shuffle : boolean, optional\n",
      "            If True the order of the dataset is shuffled to avoid having\n",
      "            images of the same person grouped.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=0)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        download_if_missing : optional, True by default\n",
      "            If False, raise a IOError if the data is not locally available\n",
      "            instead of trying to download the data from the source site.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        An object with the following attributes:\n",
      "        \n",
      "        data : numpy array of shape (400, 4096)\n",
      "            Each row corresponds to a ravelled face image of original size\n",
      "            64 x 64 pixels.\n",
      "        \n",
      "        images : numpy array of shape (400, 64, 64)\n",
      "            Each row is a face image corresponding to one of the 40 subjects\n",
      "            of the dataset.\n",
      "        \n",
      "        target : numpy array of shape (400, )\n",
      "            Labels associated to each face image. Those labels are ranging from\n",
      "            0-39 and correspond to the Subject IDs.\n",
      "        \n",
      "        DESCR : string\n",
      "            Description of the modified Olivetti Faces Dataset.\n",
      "        \n",
      "        Notes\n",
      "        ------\n",
      "        \n",
      "        This dataset consists of 10 pictures each of 40 individuals. The original\n",
      "        database was available from (now defunct)\n",
      "        \n",
      "            http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\n",
      "        \n",
      "        The version retrieved here comes in MATLAB format from the personal\n",
      "        web page of Sam Roweis:\n",
      "        \n",
      "            http://www.cs.nyu.edu/~roweis/\n",
      "    \n",
      "    remove(path, *, dir_fd=None)\n",
      "        Remove a file (same as unlink()).\n",
      "        \n",
      "        If dir_fd is not None, it should be a file descriptor open to a directory,\n",
      "          and path should be relative; path will then be relative to that directory.\n",
      "        dir_fd may not be implemented on your platform.\n",
      "          If it is unavailable, using it will raise a NotImplementedError.\n",
      "\n",
      "DATA\n",
      "    FACES = RemoteFileMetadata(filename='olivettifaces.mat',...c62d3e1266e...\n",
      "    MODULE_DOCS = 'Modified Olivetti faces dataset.\\n\\nThe original d...il...\n",
      "\n",
      "FILE\n",
      "    c:\\programdata\\anaconda3\\lib\\site-packages\\sklearn\\datasets\\olivetti_faces.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For help on the dataset of sklearn library\n",
    "help(sklearn.datasets.olivetti_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.12547421 -0.08789608 -0.07544964 ... -0.19328208 -0.18543996\n",
      "  -0.19439802]\n",
      " [ 0.07567523  0.04871595  0.04624351 ... -0.19328208 -0.19549021\n",
      "  -0.19963364]\n",
      " [-0.11397997 -0.04418021  0.01978849 ... -0.2075001  -0.20051533\n",
      "  -0.19963364]\n",
      " ...\n",
      " [ 0.1388936   0.1306832   0.16793665 ... -0.16484606 -0.20051533\n",
      "  -0.15251322]\n",
      " [-0.25765812 -0.28461736 -0.32941788 ...  0.29013026  0.33717304\n",
      "   0.371047  ]\n",
      " [ 0.16188207  0.037787   -0.2500528  ...  0.04368475  0.05074093\n",
      "   0.0935601 ]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data = fetch_olivetti_faces()\n",
    "\n",
    "X = data.data\n",
    "Y = data.target\n",
    "\n",
    "#Normalize the dataset\n",
    "# X = X.T\n",
    "X = (X-np.mean(X, axis=0, keepdims = True))/(np.max(X, axis=0, keepdims = True)-np.min(X, axis = 0, keepdims = True))\n",
    "\n",
    "#map labels 0 or 1\n",
    "Y[Y<20]=0\n",
    "Y[Y>=20]=1\n",
    "\n",
    "# choose indices from 0 and 1 seperately\n",
    "class0 = [i for i in range(0, len(Y)) if(Y[i]==0)]\n",
    "class1 = [i for i in range(0, len(Y)) if(Y[i]==1)]\n",
    "\n",
    "# sampling indices from 0 and 1 classes seperately\n",
    "train_indices = random.sample(class0, int(0.7*len(class0))) + random.sample(class1, int(0.7*len(class1)))\n",
    "\n",
    "# test indices: all indices not used in training\n",
    "test_indices = [i for i in range(0, len(Y)) if i not in train_indices]\n",
    "\n",
    "# training data\n",
    "X_train = X[train_indices]\n",
    "Y_train = Y[train_indices].reshape(len(train_indices), 1)\n",
    "\n",
    "# print(X_train.shape, len(train_indices))\n",
    "# testing data\n",
    "X_test = X[test_indices]\n",
    "Y_test = Y[test_indices].reshape(len(test_indices), 1)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X(Features) is: (280, 4096)\n",
      "The shape of Y(Target values) is: (280, 1)\n",
      "Number of training examples: 4096\n"
     ]
    }
   ],
   "source": [
    "shape_X = X_train.shape\n",
    "# Y = Y.reshape(1,Y.shape[0])\n",
    "shape_Y = Y_train.shape\n",
    "m = X_train.shape[1]  # training set size\n",
    "\n",
    "# Type: numpy array, validate using function type(X), type(Y)\n",
    "print ('The shape of X(Features) is: ' + str(shape_X))\n",
    "print ('The shape of Y(Target values) is: ' + str(shape_Y))\n",
    "print ('Number of training examples:', m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y, h_layers,h1,h2):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    h_layers --number of hidden layers\n",
    "    \"\"\" \n",
    "    n_x = X.shape[0] # size of input layer\n",
    "    n_h1 = h1#20\n",
    "    n_h2 = h2#10\n",
    "    n_y = Y.shape[0] # size of output layer\n",
    "    ### END CODE HERE ###\n",
    "    return list([n_x, n_h1, n_h2, n_y])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize parameters of 3 layer Neural network with 2 hidden layers\n",
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Input : layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    Output: python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 ...  0.0104185  -0.00023971\n",
      "   0.02130309]\n",
      " [-0.00476411  0.0027109  -0.009184   ...  0.00827621 -0.0055955\n",
      "   0.00721238]\n",
      " [ 0.00702008 -0.01196892 -0.00752024 ...  0.00742033  0.00777721\n",
      "  -0.02044101]\n",
      " ...\n",
      " [ 0.01526746  0.01191022  0.02568552 ...  0.0135046   0.01122616\n",
      "  -0.01048173]\n",
      " [ 0.00369967 -0.00146788  0.00990617 ... -0.01298496 -0.00805707\n",
      "   0.0095744 ]\n",
      " [-0.01156654 -0.00880428  0.00863347 ...  0.0019892   0.00733251\n",
      "  -0.00681849]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[ 8.76957543e-03  4.96788615e-03  1.27143067e-03 -1.97009876e-03\n",
      "   1.00570245e-03 -2.90507262e-03 -1.34373916e-03  4.19959653e-03\n",
      "  -1.15970427e-02  7.78898143e-03 -2.57557045e-03 -1.47448667e-03\n",
      "   1.47767128e-02 -3.52975107e-03 -1.40603763e-03 -6.75128340e-03\n",
      "  -4.92942209e-03  9.96221569e-03 -4.24367873e-03 -1.55820179e-03]\n",
      " [-5.14772688e-03  3.59909944e-03  6.32478900e-03 -7.54779869e-04\n",
      "  -3.95558443e-03 -2.47466311e-03 -1.14147811e-02 -2.33927431e-03\n",
      "   4.80442037e-03 -1.43624677e-02 -2.94120659e-02 -3.38355680e-03\n",
      "   4.61898443e-03  6.97209160e-03 -9.04064340e-03  1.29823821e-02\n",
      "   1.34304373e-02 -1.07261242e-02  1.14050534e-02 -5.24255014e-04]\n",
      " [ 1.44341128e-03 -1.21400326e-02  1.41074677e-02 -1.24578843e-03\n",
      "  -2.47260797e-03  2.57734270e-04  1.00178640e-02 -1.25111031e-02\n",
      "  -4.16236730e-03 -6.46949497e-03 -3.55864340e-03  1.24811387e-02\n",
      "  -1.36494428e-03 -1.36626825e-02 -1.46571226e-02 -2.00141248e-02\n",
      "  -1.77295386e-02 -3.44823799e-03  2.17026947e-02 -5.51536406e-03]\n",
      " [ 3.05998629e-03  1.46517216e-03 -3.15906981e-02  2.16710482e-02\n",
      "  -7.03797957e-03  2.57084893e-03 -1.74534525e-02  3.67349417e-03\n",
      "  -7.26212216e-03  2.82139909e-03  5.68328500e-04 -1.89504990e-02\n",
      "   5.04033673e-03 -3.77362164e-03 -5.16239571e-04 -7.54313456e-03\n",
      "  -2.84840468e-03  1.21577179e-02 -1.01292827e-02  3.47308668e-03]\n",
      " [ 3.69929127e-03  7.48610783e-03 -4.05768022e-03  4.52708899e-04\n",
      "   1.42112052e-02  6.85065651e-03  1.82365842e-02  3.61798264e-03\n",
      "  -8.21732309e-03  7.15970430e-03 -1.19902949e-02 -2.02329212e-04\n",
      "   2.25656081e-03  1.05447485e-02 -1.14630404e-03  2.25789194e-02\n",
      "  -1.32503136e-02  1.08051426e-02  4.14853833e-03  8.15616383e-03]\n",
      " [ 9.65665685e-03 -1.70523152e-02 -7.82596133e-03  1.68467080e-03\n",
      "   4.91625583e-04 -7.54987051e-04 -4.83991045e-03  3.86970644e-03\n",
      "   2.00961017e-02  5.06247772e-03 -3.58020088e-03 -3.80153709e-03\n",
      "  -5.70167544e-03  5.21003785e-03 -9.32942922e-04 -1.01069083e-03\n",
      "   1.44017028e-03  7.66993206e-04  2.88004860e-02  2.34538343e-03]\n",
      " [-5.09259023e-03  3.90140643e-03  3.60073946e-03  1.81485512e-02\n",
      "   2.33194365e-03  1.04770541e-02  9.49585126e-04  3.60606005e-03\n",
      "   2.26702443e-03  2.36738294e-03  9.45087172e-03  1.37252111e-02\n",
      "  -3.57725082e-03  6.97073894e-03 -7.57647091e-03  2.23178821e-02\n",
      "   4.32121846e-03 -1.44306910e-02  1.98021838e-02  4.81236660e-03]\n",
      " [-7.85104871e-03  5.78373594e-03  1.16236268e-02 -1.32575546e-02\n",
      "  -3.05759380e-03 -1.19887935e-03 -6.49447495e-03 -9.95050042e-03\n",
      "  -1.21929807e-02  8.58787401e-03 -8.80643649e-03 -1.25663552e-05\n",
      "   4.05678583e-03  8.02526485e-03 -2.80403632e-03 -1.73593029e-03\n",
      "   1.57365523e-02  8.80538218e-03 -8.69117637e-04  7.41977842e-03]\n",
      " [ 2.07838606e-02 -1.39765913e-02 -3.25844386e-04 -9.47638488e-03\n",
      "  -5.21759513e-03 -1.61675708e-02  7.23886352e-03  5.71016348e-03\n",
      "   2.32382441e-03 -5.23914308e-04  5.21891792e-03  1.35432126e-02\n",
      "  -3.85999880e-03  1.09385376e-02 -7.17224226e-03 -4.80118184e-03\n",
      "   8.72009841e-03  2.38879740e-03  5.55087720e-03 -2.16830678e-03]\n",
      " [ 2.81016179e-03 -2.49566974e-03  9.72824360e-03  8.72679282e-03\n",
      "   1.56167236e-02  6.22829794e-03 -7.72977365e-03 -2.31275144e-02\n",
      "   2.96623769e-03 -4.85761953e-03 -1.27778630e-02  2.57883213e-02\n",
      "  -8.49149134e-03 -5.41747501e-03 -1.23162334e-02 -1.00307795e-03\n",
      "   8.51155655e-03  8.46009911e-03 -2.45669330e-03 -8.13436269e-05]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W3 = [[ 0.00204191 -0.01158691 -0.005668   -0.00191661 -0.0091336  -0.00159708\n",
      "  -0.00454402 -0.01709472 -0.00769455  0.00194981]]\n",
      "b3 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(layer_dims)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "print(\"W3 = \" + str(parameters[\"W3\"]))\n",
    "print(\"b3 = \" + str(parameters[\"b3\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Forward and Backward Propagation ####\n",
    "\n",
    "**Question**: Implement `forward_propagation()`.\n",
    "\n",
    "**Instructions**:\n",
    "- Look above at the mathematical representation of your classifier.\n",
    "- You can use the function `np.tanh()`. It is part of the numpy library.\n",
    "- The steps you have to implement are:\n",
    "    1. Retrieve each parameter from the dictionary \"parameters\" (which is the output of `initialize_parameters()`) by using `parameters[\"..\"]`.\n",
    "    2. Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set).\n",
    "- Values needed in the backpropagation are stored in \"`cache`\". The `cache` will be given as an input to the backpropagation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "activations_forward = {\n",
    "    \"tanh\": lambda x: np.tanh(x),\n",
    "    \"sigmoid\": lambda x: 1.0/(1+np.exp(-x)),\n",
    "    \"relu\": lambda x: np.maximum(0, x)\n",
    "}\n",
    "activations_backward = {\n",
    "    \"tanh\": lambda x: 1-np.power(x, 2),\n",
    "    \"sigmoid\": lambda x: np.multiply(x, 1-x),\n",
    "    \"relu\": lambda x: np.greater(x, 0).astype(int)\n",
    "}\n",
    "\n",
    "def forward_propagation(X, parameters, functions=[\"tanh\", \"tanh\", \"sigmoid\"]):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = activations_forward[functions[0]](Z1)\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = activations_forward[functions[1]](Z2)\n",
    "    Z3 = np.dot(W3,A2) + b3\n",
    "    A3 = activations_forward[functions[2]](Z3)\n",
    "\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2,\n",
    "             \"Z3\": Z3,\n",
    "             \"A3\": A3}\n",
    "    \n",
    "    return A3, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "#Compute Cost\n",
    "def compute_cost(A_final, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computes the cost \n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the final activation\n",
    "    Y -- \"true\" labels vector\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 , b2 , W3 , b3\n",
    "    \n",
    "    Returns:\n",
    "    cost \n",
    "    \"\"\"\n",
    "    #print(\"A_final\",(A_final))\n",
    "    m = Y.shape[1] # number of example\n",
    "    # Compute the cost\n",
    "    \n",
    "    logprobs = np.multiply(np.log(A_final), Y) + np.multiply((1 - Y), np.log(1 - A_final))\n",
    "    cost = -np.sum(logprobs) / m\n",
    "    \n",
    "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
    "                                # E.g., turns [[17]] into 17 \n",
    "    return cost\n",
    "\n",
    "def dCost(A, y):\n",
    "    return np.divide(1-y, 1-A) - np.divide(y, A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: backward_propagation\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y, functions=[\"tanh\", \"tanh\", \"sigmoid\"]):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" , \"A2\" , Z3 , A3.\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 , W2 and W3 from the dictionary \"parameters\".\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # Retrieve also A1 , A2 and A3 from dictionary \"cache\".\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    A3 = cache[\"A3\"]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2 , dW3, db3. \n",
    "    dZ3 = np.multiply(dCost(A3, Y), activations_backward[functions[2]](A3))\n",
    "    dW3 = 1/m*(np.dot(dZ3,A2.T))\n",
    "    db3 = 1/m*(np.sum(dZ3,axis=1, keepdims=True))\n",
    "    \n",
    "    dZ2 = np.multiply(np.dot(W3.T,dZ3),activations_backward[functions[1]](A2))\n",
    "    dW2 = 1/m*(np.dot(dZ2,A1.T))\n",
    "    db2 = 1/m*(np.sum(dZ2,axis=1, keepdims=True))\n",
    "    \n",
    "    dZ1 = np.multiply(np.dot(W2.T,dZ2),activations_backward[functions[0]](A1))\n",
    "    dW1 = 1/m*(np.dot(dZ1,X.T))\n",
    "    db1 = 1/m*(np.sum(dZ1,axis=1, keepdims=True))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2,\n",
    "             \"dW3\": dW3,\n",
    "             \"db3\": db3}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate = 0.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    dW3 = grads[\"dW3\"]\n",
    "    db3 = grads[\"db3\"]\n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    W3 = W3 - learning_rate*dW3\n",
    "    b3 = b3 - learning_rate*db3\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: nn_model\n",
    "\n",
    "def nn_model(X, Y, h1=20, h2=10, functions=[\"tanh\", \"tanh\", \"sigmoid\"], num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    layer_dims = layer_sizes(X, Y , 2, h1, h2)\n",
    "    \n",
    "    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n",
    "    ### START CODE HERE ### (≈ 5 lines of code)\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    costList = []\n",
    "    for i in range(0, num_iterations+1):\n",
    "        \n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A3, cache\".\n",
    "        A3, cache = forward_propagation(X,parameters, functions)\n",
    "        \n",
    "        # Cost function. Inputs: \"A3, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A3,Y,parameters)\n",
    "        costList.append(cost)\n",
    "        \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(parameters,cache,X,Y, functions)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters,grads)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    plt.plot(range(0, num_iterations+1), costList)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(parameters, X, functions=[\"tanh\", \"tanh\", \"sigmoid\"]):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A3, cache = forward_propagation(X,parameters, functions)\n",
    "    predictions = 1*(A3>0.5)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693145\n",
      "Cost after iteration 100: 0.693108\n",
      "Cost after iteration 200: 0.691832\n",
      "Cost after iteration 300: 0.314294\n",
      "Cost after iteration 400: 0.018946\n",
      "Cost after iteration 500: 0.005336\n",
      "Cost after iteration 600: 0.002960\n",
      "Cost after iteration 700: 0.002034\n",
      "Cost after iteration 800: 0.001548\n",
      "Cost after iteration 900: 0.001248\n",
      "Cost after iteration 1000: 0.001046\n",
      "Cost after iteration 1100: 0.000899\n",
      "Cost after iteration 1200: 0.000788\n",
      "Cost after iteration 1300: 0.000701\n",
      "Cost after iteration 1400: 0.000631\n",
      "Cost after iteration 1500: 0.000574\n",
      "Cost after iteration 1600: 0.000525\n",
      "Cost after iteration 1700: 0.000484\n",
      "Cost after iteration 1800: 0.000449\n",
      "Cost after iteration 1900: 0.000419\n",
      "Cost after iteration 2000: 0.000392\n",
      "Cost after iteration 2100: 0.000368\n",
      "Cost after iteration 2200: 0.000347\n",
      "Cost after iteration 2300: 0.000328\n",
      "Cost after iteration 2400: 0.000313\n",
      "Cost after iteration 2500: 0.000300\n",
      "Cost after iteration 2600: 0.000288\n",
      "Cost after iteration 2700: 0.000277\n",
      "Cost after iteration 2800: 0.000266\n",
      "Cost after iteration 2900: 0.000257\n",
      "Cost after iteration 3000: 0.000249\n",
      "Cost after iteration 3100: 0.000240\n",
      "Cost after iteration 3200: 0.000233\n",
      "Cost after iteration 3300: 0.000226\n",
      "Cost after iteration 3400: 0.000219\n",
      "Cost after iteration 3500: 0.000213\n",
      "Cost after iteration 3600: 0.000207\n",
      "Cost after iteration 3700: 0.000202\n",
      "Cost after iteration 3800: 0.000197\n",
      "Cost after iteration 3900: 0.000192\n",
      "Cost after iteration 4000: 0.000187\n",
      "Cost after iteration 4100: 0.000182\n",
      "Cost after iteration 4200: 0.000178\n",
      "Cost after iteration 4300: 0.000174\n",
      "Cost after iteration 4400: 0.000170\n",
      "Cost after iteration 4500: 0.000167\n",
      "Cost after iteration 4600: 0.000163\n",
      "Cost after iteration 4700: 0.000160\n",
      "Cost after iteration 4800: 0.000156\n",
      "Cost after iteration 4900: 0.000153\n",
      "Cost after iteration 5000: 0.000150\n",
      "Training over\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build a model with a n_h-dimensional hidden layer# with nodes 20,10\n",
    "activations = [\"relu\", \"relu\", \"sigmoid\"]\n",
    "\n",
    "parameters = nn_model(X_train.T, Y_train.T, 20, 5, activations, num_iterations = 5000, print_cost=True)\n",
    "print(\"Training over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95%\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "predictions = predict(parameters, X_test.T, activations)\n",
    "#print(predictions)\n",
    "# print( Y_test.shape, predictions.shape)\n",
    "# np.dot(Y_test, predictions)\n",
    "print ('Accuracy: %d' % float((np.dot(predictions, Y_test) + np.dot(1-predictions, 1-Y_test))/float(Y_test.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 92.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,Y_train)\n",
    "print(\"LogisticRegression\", model.score(X_test,Y_test)*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
